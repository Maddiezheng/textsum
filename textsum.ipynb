{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summarization</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>深圳地铁将设立ＶＩＰ头等车厢　买双倍票可享坐票</td>\n",
       "      <td>南都讯　记者刘凡　周昌和　任笑一　继推出日票后，深圳今后将设地铁ＶＩＰ头等车厢，设坐票制。昨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中国西部是地球上主要干旱带之一，妇女是当地劳动力．．．</td>\n",
       "      <td>同心县地处宁夏中部干旱带的核心区，　冬寒长，春暖迟，夏热短，秋凉早，干旱少雨，蒸发强烈，风大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>思源焦点公益基金救助孩子：永康</td>\n",
       "      <td>不满一岁的永康是个饱经病痛折磨的孩子，２０１１年７月５日出生的他，患有先天性心脏病、疝气，一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>康师傅回应转卖废弃茶叶：下家承诺用废料做枕头</td>\n",
       "      <td>就废弃茶叶被转手事件发声明本报讯（记者刘俊）　“我们也是受害者！”昨日，有媒体报道称康师傅...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>活动时间：</td>\n",
       "      <td>·奖励办法：率先提交的前１００个创意项目，经评估，可优先资助实施。·咨询电话：０１０－６７...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>５．１２灾后重建资助项目投票评选</td>\n",
       "      <td>２００９年８月，《２００９中国慈善导航行动》第一季正式启动，此档由ＣＣＴＶ－１２《大家看法》...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>以书为友，知行合一—２０１２年小桔灯湖北站</td>\n",
       "      <td>２０１２年东风标致小桔灯乡村小学图书馆计划于６月２３日－２９日在湖北省武汉市新洲区凤凰镇郭岗...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>博爱周活动时间：</td>\n",
       "      <td>１、纪念汶川地震１周年２、纪念５月１２日国家首个“防灾减灾日”３、纪念索尔弗利诺战役１５...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>“金葵花”羌族少儿合唱团公益活动</td>\n",
       "      <td>２、９月２２日，与阿坝州及茂县各方人士商议在四川省阿坝藏族羌族自治州组建“羌族少儿合唱团”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>视频征集入围短片</td>\n",
       "      <td>从汽车发明到现在，平均每天因事故死亡３３００人左右，相当于１０架大型客机坠毁。这些事故中，９...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>１５所学校过初选　甘肃１１９７名山里娃有望穿上新鞋</td>\n",
       "      <td>甘肃首批１６所偏远山区学校中，有１５所学校的１１９７名困难学生有望获得爱心运动鞋。截至目前...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>２０１０地球一小时</td>\n",
       "      <td>野草的经费６０％来自基金会赞助，另外４０％是通过策划项目赚来的。２００６年一年，野草所有人一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>２０１２年＂中国爱心城市＂公益活动举行新闻发布会</td>\n",
       "      <td>６月１９日，《２０１２年度“中国爱心城市”公益活动新闻发布会》在京举行。中华社会救助基金会理...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>南山奶粉５批次含强致癌物　光明奶油上“黑榜”</td>\n",
       "      <td>信息时报讯　（记者　何小敏　李星慧）南山奶粉５批次含强致癌物！前日深夜，广州市工商局在官网公...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>因为有你爱驻我家　－　六一娃娃礼物计划</td>\n",
       "      <td>山东日照安康家园安置点集中了来自四川灾区的５２２名孩子，其中３３８人为地震孤儿。在六一儿童节...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>深圳拟分类救助流浪乞讨者　将劝返职业乞讨人员</td>\n",
       "      <td>７月２日，深圳市法制办在其网站公布了《深圳经济特区社会救助条例》（以下简称《条例》），再次公...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>国籍保障公民基本权利</td>\n",
       "      <td>目前，这些数以千计的在英马来西亚华人，既丧失了马来西亚国籍，又无法取得英国国籍，成为无国籍“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>徐大发：“活祭”老父求救助是损德的恶炒</td>\n",
       "      <td>２０１２年７月１６日上午，杜甫江阁附近，一位７２岁的老人在风雨中被儿“活祭”。上午１０点，记...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>头枕砸窗撬窗均被证不靠谱　汽车玻璃贴膜更难砸</td>\n",
       "      <td>头枕砸窗靠不住　汽车逃生器保险继“头枕砸窗”被众多汽车专家和媒体证实为“不靠谱的逃生方法”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>思源焦点公益基金受助孩子：坚强的萍萍</td>\n",
       "      <td>２００８年８月的一个清晨，Ａ市女子某医院大院的水淹地里，一个熟睡的女婴被人发现。出生仅两天的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>“信为本、孝为先”　敬老爱老公益计划在京启动</td>\n",
       "      <td>７月１０日，敬老爱老公益计划在京启动。此次活动的主题为＂信为本、孝为先＂，由中信银行与《２１...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>黄怒波：１０年内捐５００亿　不再信任官办慈善机构</td>\n",
       "      <td>中坤集团董事长黄怒波在接受《福布斯》专访时表示：“现在慈善大多是官办，我比较抵触，捐钱好像还...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>低碳经济呼唤戒除奢侈消费</td>\n",
       "      <td>无论是政协会议的“一号提案”，还是各省区的人大代表分组讨论，“低碳”成为今年两会的最热门话题...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>医保缴费年限将各地互认并累计　将研究延迟退休</td>\n",
       "      <td>［提要］　《关于批转社会保障“十二五”规划纲要的通知》提出：要落实医疗保险关系转移接续办法，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>“送双运动鞋”项目公示：思源焦点基金首批捐赠</td>\n",
       "      <td>“思源ｂ焦点”公益基金的工作人员和志愿者一起将９２２双鞋打包志愿者为＂给孩子送双运动鞋＂公...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>社会公众碳汇同行个人捐助计划：一人捐赠一棵树、争做绿色公民</td>\n",
       "      <td>搜狐公益频道与中国绿化基金会联合发起的“搜狐网友金山岭植下许愿树”公益活动顺利开展。网上报名...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>从问题奶粉、地沟油到假消毒餐具，更大的问题还在于道德的溃败、信任的失守［</td>\n",
       "      <td>“基本信任”流失、商业道德溃败的结果，是以邻为壑，人与人之间互设陷阱：你给我吃地沟油，我给你...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>崔永元请农民工吃饭：自己掏腰包　不代表任何人</td>\n",
       "      <td>崔永元谈请客：这是我个人表达谢意　不代表任何人四川新闻网北京７月３０日讯（记者　张燕）　京...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>英国游客性骚扰中国女孩　律师称可能轻罚惹众怒</td>\n",
       "      <td>近日，《环球时报》英文版的一篇题为《Ｂｒｉｔ　Ｂｅａｔｅｎ　Ａｆｔｅｒ　Ａｌｌｅｇｅｄ　Ｓｅ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>浙江９名师生支教四川　凉山深处上学娃（图）</td>\n",
       "      <td>６月底，我们浙江传媒学院９名师生从杭州来到四川省凉山彝族自治州甘洛县尼尔觉乡牛吾村小学，进行...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>思源焦点公益基金救助孩子：童童</td>\n",
       "      <td>２０１１年５月１０日，出生仅３天的童童被遗弃在Ａ市一所酒店的后门。刚出生的她，从外表上看不出...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>爸爸见义勇为遭冷遇　１２岁儿子劝其“以后别管”</td>\n",
       "      <td>本报讯（实习记者　靳鸽　记者　何杰）　见一男子抢劫女子的包，魏先生追上去帮忙将男子制住，将包...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>水井坊遗址（传统手工技艺类）</td>\n",
       "      <td>探访团队来到活动首站——四川水井坊酒厂，就水井坊的酿造技艺采访白酒酿酒专家、水井坊酒传统酿造...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>为便于交流和及时了解活动信息，请各社团开通微博，开通办法：</td>\n",
       "      <td>１２月２２日，中华环境保护基金会秘书长李伟、中国环境报社副社长李瑞农、共青团中国人民大学委员...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>参加过的战争　松山战役</td>\n",
       "      <td>姓名　范松池部队信息　远征军第八军１９４４年，做为一名炮兵的范松池参加了当年的松山战役。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>志愿者经历打开升学之门</td>\n",
       "      <td>北京市有关部门联合制定意见，在全市建立和完善学生志愿服务长效机制。他们在校期间参加志愿服务的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>红丝带神州行－预防艾滋病列车宣传活动</td>\n",
       "      <td>中华红丝带基金开展的公益活动２００７世界艾滋病日－十大高校公益宣传活动此次活动主要针对的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>大学生绿色偶像评选</td>\n",
       "      <td>２００９年１１月１６日，“大学生绿色偶像评选”活动正式启动。作为“第四届全国大学生环保创意大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>益调查：你是否担心血荒常态化？</td>\n",
       "      <td>６月１２日，卫生部发布《医疗机构临床用血管理办法》，旨在通过“节流”保障临床用血需求和血液安...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>苍南回应福利院铁链拴养孤儿　将查违法渎职行为</td>\n",
       "      <td>浙江温州苍南官方１日对媒体报道“苍南福利院铁链拴男童”一事进行调查，回应称报道情况基本属实，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>２０１１“芯世界”公益创新奖获奖项目</td>\n",
       "      <td>２０１１年７月３日－８日，芯世界组委会带领２０１１“芯世界”公益创新奖获奖机构代表在韩国开始...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>北京人＂非常幸福＂仅０．０８％　食品安全心理压力大</td>\n",
       "      <td>昨天，在北京市人口计生委举办的家庭和谐人口主题报告会上，一份针对本市家庭幸福指数的研究结果发...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>上海地铁请女性自重　女志愿者用行为艺术示抗议</td>\n",
       "      <td>上海地铁请女性“自重”引争议女志愿者行为艺术示抗议新闻源：＠女权之声：为抗议＠上海地铁二...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>全国人大常委会组成人员：饮用水应有专管部门</td>\n",
       "      <td>全国人大常委会组成人员分组审议时建议饮用水安全应有专管部门本报北京６月２８日电（记者白龙...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>“白领黑苹果—夏花会”简介</td>\n",
       "      <td>零点青年公益创业发展中心（ＹＥＳ）发起“白领黑苹果”【春夏秋冬】四季会系列活动，旨在联系关注...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>广发希望慈善基金爱心足迹</td>\n",
       "      <td>２０１０年１０月，广发希望慈善基金的志愿者代表们带着代表希望的“心愿卡”深入到云南楚雄彝族自...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>世界第７０亿人在菲律宾降生</td>\n",
       "      <td>世界将在本月３１日迎来第７０亿人口，而在３１日凌晨，将成为象征性的全球第７０亿名成员之一的婴...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>２０１０索尼海外学生交流精彩瞬间</td>\n",
       "      <td>索尼海外学生交流计划（Ｓｏｎｙ　Ｓｔｕｄｅｎｔ　Ｐｒｏｊｅｃｔ　Ａｂｒｏａｄ　（Ｃｈｉｎａ）...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>思源焦点公益基金救助孩子：爱笑的纯纯</td>\n",
       "      <td>爱纯是一个出生在中国北部地区的小女孩。笑容背后，她还是一个患有心脏病的孤儿。她的心脏室间隔缺...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>联合国艾滋病规划署</td>\n",
       "      <td>欢迎各企业动员本企业员工和／或合作伙伴一同参与本次活动并进行捐赠。每位参与活动人员的注册费用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>中国成象牙消费最大国家　消费者成间接杀手</td>\n",
       "      <td>国人出国旅游买回象牙属违法，三年来涉案金额过亿；私带象牙制品入境，轻则罚没重则追究刑事责任。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>是在瑞士日内瓦缔结的关于保护平民和战争受难者的一系列国际公约的总称…［</td>\n",
       "      <td>在许多情况下，只有公正和中立的组织才有能力为被武装冲突波及的人们提供帮助。例如，２００３年３...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>“小桔灯”乡村小学图书馆计划活动介绍</td>\n",
       "      <td>活动主办：２１世纪经济报道战略合作：东风标致５０８我们深信：知识改变命运。乡村的孩子们跟...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>报名参加四地线下观鸟活动</td>\n",
       "      <td>“爱鸟周”源于１９８１年，最初为保护迁徙于中日两国间的候鸟而设立。１９９２年国务院批准的《陆...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>人社部：延迟退休拟采取差别政策</td>\n",
       "      <td>延迟退休拟采取差别政策人社部表示将以适当方式充分听取各方意见，并以＂小步慢走＂的方式实施...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>山水自然保护中心的大熊猫保护观</td>\n",
       "      <td>导读：几万年前，世界野外大熊猫种群分布最南端在缅甸和越南北部北纬１９度一带，由于气候的变迁和...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>合肥毁容少女周岩索赔１１６万　提交民事诉讼请求</td>\n",
       "      <td>今年５月１０日，备受关注的“合肥市少女毁容案”在合肥市市包河区法院第一法庭公开宣判，但此案附...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>潘石屹携旗下基金向“给孩子送双运动鞋”捐款</td>\n",
       "      <td>ＳＯＨＯＣＨＩＮＡ向项目捐赠３３８００元“给孩子送双运动鞋”公益行动由中华少年儿童慈善救助...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>红会首次向７－２１遇难者发放慰问金　烈士每人３万</td>\n",
       "      <td>红会首次发遇难者慰问金遇难群众每人１万　因公殉职每人２万　烈士每人３万本报讯（记者李秋萌...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>“娃娃，唱歌给你听”儿童音乐行动刚启动就收到很多明星的</td>\n",
       "      <td>截止目前，通过搜狐官网和溜达音乐网提交的参赛词、曲作品已达５００多首。因参赛者的强烈要求，...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           summarization  \\\n",
       "0                深圳地铁将设立ＶＩＰ头等车厢　买双倍票可享坐票   \n",
       "1            中国西部是地球上主要干旱带之一，妇女是当地劳动力．．．   \n",
       "2                        思源焦点公益基金救助孩子：永康   \n",
       "3                 康师傅回应转卖废弃茶叶：下家承诺用废料做枕头   \n",
       "4                                  活动时间：   \n",
       "5                       ５．１２灾后重建资助项目投票评选   \n",
       "6                  以书为友，知行合一—２０１２年小桔灯湖北站   \n",
       "7                               博爱周活动时间：   \n",
       "8                       “金葵花”羌族少儿合唱团公益活动   \n",
       "9                               视频征集入围短片   \n",
       "10             １５所学校过初选　甘肃１１９７名山里娃有望穿上新鞋   \n",
       "11                             ２０１０地球一小时   \n",
       "12              ２０１２年＂中国爱心城市＂公益活动举行新闻发布会   \n",
       "13                南山奶粉５批次含强致癌物　光明奶油上“黑榜”   \n",
       "14                   因为有你爱驻我家　－　六一娃娃礼物计划   \n",
       "15                深圳拟分类救助流浪乞讨者　将劝返职业乞讨人员   \n",
       "16                            国籍保障公民基本权利   \n",
       "17                   徐大发：“活祭”老父求救助是损德的恶炒   \n",
       "18                头枕砸窗撬窗均被证不靠谱　汽车玻璃贴膜更难砸   \n",
       "19                    思源焦点公益基金受助孩子：坚强的萍萍   \n",
       "20                “信为本、孝为先”　敬老爱老公益计划在京启动   \n",
       "21              黄怒波：１０年内捐５００亿　不再信任官办慈善机构   \n",
       "22                          低碳经济呼唤戒除奢侈消费   \n",
       "23                医保缴费年限将各地互认并累计　将研究延迟退休   \n",
       "24                “送双运动鞋”项目公示：思源焦点基金首批捐赠   \n",
       "25         社会公众碳汇同行个人捐助计划：一人捐赠一棵树、争做绿色公民   \n",
       "26  从问题奶粉、地沟油到假消毒餐具，更大的问题还在于道德的溃败、信任的失守［   \n",
       "27                崔永元请农民工吃饭：自己掏腰包　不代表任何人   \n",
       "28                英国游客性骚扰中国女孩　律师称可能轻罚惹众怒   \n",
       "29                 浙江９名师生支教四川　凉山深处上学娃（图）   \n",
       "..                                   ...   \n",
       "70                       思源焦点公益基金救助孩子：童童   \n",
       "71               爸爸见义勇为遭冷遇　１２岁儿子劝其“以后别管”   \n",
       "72                        水井坊遗址（传统手工技艺类）   \n",
       "73         为便于交流和及时了解活动信息，请各社团开通微博，开通办法：   \n",
       "74                           参加过的战争　松山战役   \n",
       "75                           志愿者经历打开升学之门   \n",
       "76                    红丝带神州行－预防艾滋病列车宣传活动   \n",
       "77                             大学生绿色偶像评选   \n",
       "78                       益调查：你是否担心血荒常态化？   \n",
       "79                苍南回应福利院铁链拴养孤儿　将查违法渎职行为   \n",
       "80                    ２０１１“芯世界”公益创新奖获奖项目   \n",
       "81             北京人＂非常幸福＂仅０．０８％　食品安全心理压力大   \n",
       "82                上海地铁请女性自重　女志愿者用行为艺术示抗议   \n",
       "83                 全国人大常委会组成人员：饮用水应有专管部门   \n",
       "84                         “白领黑苹果—夏花会”简介   \n",
       "85                          广发希望慈善基金爱心足迹   \n",
       "86                         世界第７０亿人在菲律宾降生   \n",
       "87                      ２０１０索尼海外学生交流精彩瞬间   \n",
       "88                    思源焦点公益基金救助孩子：爱笑的纯纯   \n",
       "89                             联合国艾滋病规划署   \n",
       "90                  中国成象牙消费最大国家　消费者成间接杀手   \n",
       "91   是在瑞士日内瓦缔结的关于保护平民和战争受难者的一系列国际公约的总称…［   \n",
       "92                    “小桔灯”乡村小学图书馆计划活动介绍   \n",
       "93                          报名参加四地线下观鸟活动   \n",
       "94                       人社部：延迟退休拟采取差别政策   \n",
       "95                       山水自然保护中心的大熊猫保护观   \n",
       "96               合肥毁容少女周岩索赔１１６万　提交民事诉讼请求   \n",
       "97                 潘石屹携旗下基金向“给孩子送双运动鞋”捐款   \n",
       "98              红会首次向７－２１遇难者发放慰问金　烈士每人３万   \n",
       "99           “娃娃，唱歌给你听”儿童音乐行动刚启动就收到很多明星的   \n",
       "\n",
       "                                              article  \n",
       "0   南都讯　记者刘凡　周昌和　任笑一　继推出日票后，深圳今后将设地铁ＶＩＰ头等车厢，设坐票制。昨...  \n",
       "1   同心县地处宁夏中部干旱带的核心区，　冬寒长，春暖迟，夏热短，秋凉早，干旱少雨，蒸发强烈，风大...  \n",
       "2   不满一岁的永康是个饱经病痛折磨的孩子，２０１１年７月５日出生的他，患有先天性心脏病、疝气，一...  \n",
       "3   就废弃茶叶被转手事件发声明本报讯（记者刘俊）　“我们也是受害者！”昨日，有媒体报道称康师傅...  \n",
       "4   ·奖励办法：率先提交的前１００个创意项目，经评估，可优先资助实施。·咨询电话：０１０－６７...  \n",
       "5   ２００９年８月，《２００９中国慈善导航行动》第一季正式启动，此档由ＣＣＴＶ－１２《大家看法》...  \n",
       "6   ２０１２年东风标致小桔灯乡村小学图书馆计划于６月２３日－２９日在湖北省武汉市新洲区凤凰镇郭岗...  \n",
       "7   １、纪念汶川地震１周年２、纪念５月１２日国家首个“防灾减灾日”３、纪念索尔弗利诺战役１５...  \n",
       "8   ２、９月２２日，与阿坝州及茂县各方人士商议在四川省阿坝藏族羌族自治州组建“羌族少儿合唱团”...  \n",
       "9   从汽车发明到现在，平均每天因事故死亡３３００人左右，相当于１０架大型客机坠毁。这些事故中，９...  \n",
       "10  甘肃首批１６所偏远山区学校中，有１５所学校的１１９７名困难学生有望获得爱心运动鞋。截至目前...  \n",
       "11  野草的经费６０％来自基金会赞助，另外４０％是通过策划项目赚来的。２００６年一年，野草所有人一...  \n",
       "12  ６月１９日，《２０１２年度“中国爱心城市”公益活动新闻发布会》在京举行。中华社会救助基金会理...  \n",
       "13  信息时报讯　（记者　何小敏　李星慧）南山奶粉５批次含强致癌物！前日深夜，广州市工商局在官网公...  \n",
       "14  山东日照安康家园安置点集中了来自四川灾区的５２２名孩子，其中３３８人为地震孤儿。在六一儿童节...  \n",
       "15  ７月２日，深圳市法制办在其网站公布了《深圳经济特区社会救助条例》（以下简称《条例》），再次公...  \n",
       "16  目前，这些数以千计的在英马来西亚华人，既丧失了马来西亚国籍，又无法取得英国国籍，成为无国籍“...  \n",
       "17  ２０１２年７月１６日上午，杜甫江阁附近，一位７２岁的老人在风雨中被儿“活祭”。上午１０点，记...  \n",
       "18  头枕砸窗靠不住　汽车逃生器保险继“头枕砸窗”被众多汽车专家和媒体证实为“不靠谱的逃生方法”...  \n",
       "19  ２００８年８月的一个清晨，Ａ市女子某医院大院的水淹地里，一个熟睡的女婴被人发现。出生仅两天的...  \n",
       "20  ７月１０日，敬老爱老公益计划在京启动。此次活动的主题为＂信为本、孝为先＂，由中信银行与《２１...  \n",
       "21  中坤集团董事长黄怒波在接受《福布斯》专访时表示：“现在慈善大多是官办，我比较抵触，捐钱好像还...  \n",
       "22  无论是政协会议的“一号提案”，还是各省区的人大代表分组讨论，“低碳”成为今年两会的最热门话题...  \n",
       "23  ［提要］　《关于批转社会保障“十二五”规划纲要的通知》提出：要落实医疗保险关系转移接续办法，...  \n",
       "24  “思源ｂ焦点”公益基金的工作人员和志愿者一起将９２２双鞋打包志愿者为＂给孩子送双运动鞋＂公...  \n",
       "25  搜狐公益频道与中国绿化基金会联合发起的“搜狐网友金山岭植下许愿树”公益活动顺利开展。网上报名...  \n",
       "26  “基本信任”流失、商业道德溃败的结果，是以邻为壑，人与人之间互设陷阱：你给我吃地沟油，我给你...  \n",
       "27  崔永元谈请客：这是我个人表达谢意　不代表任何人四川新闻网北京７月３０日讯（记者　张燕）　京...  \n",
       "28  近日，《环球时报》英文版的一篇题为《Ｂｒｉｔ　Ｂｅａｔｅｎ　Ａｆｔｅｒ　Ａｌｌｅｇｅｄ　Ｓｅ...  \n",
       "29  ６月底，我们浙江传媒学院９名师生从杭州来到四川省凉山彝族自治州甘洛县尼尔觉乡牛吾村小学，进行...  \n",
       "..                                                ...  \n",
       "70  ２０１１年５月１０日，出生仅３天的童童被遗弃在Ａ市一所酒店的后门。刚出生的她，从外表上看不出...  \n",
       "71  本报讯（实习记者　靳鸽　记者　何杰）　见一男子抢劫女子的包，魏先生追上去帮忙将男子制住，将包...  \n",
       "72  探访团队来到活动首站——四川水井坊酒厂，就水井坊的酿造技艺采访白酒酿酒专家、水井坊酒传统酿造...  \n",
       "73  １２月２２日，中华环境保护基金会秘书长李伟、中国环境报社副社长李瑞农、共青团中国人民大学委员...  \n",
       "74  姓名　范松池部队信息　远征军第八军１９４４年，做为一名炮兵的范松池参加了当年的松山战役。...  \n",
       "75  北京市有关部门联合制定意见，在全市建立和完善学生志愿服务长效机制。他们在校期间参加志愿服务的...  \n",
       "76  中华红丝带基金开展的公益活动２００７世界艾滋病日－十大高校公益宣传活动此次活动主要针对的...  \n",
       "77  ２００９年１１月１６日，“大学生绿色偶像评选”活动正式启动。作为“第四届全国大学生环保创意大...  \n",
       "78  ６月１２日，卫生部发布《医疗机构临床用血管理办法》，旨在通过“节流”保障临床用血需求和血液安...  \n",
       "79  浙江温州苍南官方１日对媒体报道“苍南福利院铁链拴男童”一事进行调查，回应称报道情况基本属实，...  \n",
       "80  ２０１１年７月３日－８日，芯世界组委会带领２０１１“芯世界”公益创新奖获奖机构代表在韩国开始...  \n",
       "81  昨天，在北京市人口计生委举办的家庭和谐人口主题报告会上，一份针对本市家庭幸福指数的研究结果发...  \n",
       "82  上海地铁请女性“自重”引争议女志愿者行为艺术示抗议新闻源：＠女权之声：为抗议＠上海地铁二...  \n",
       "83  全国人大常委会组成人员分组审议时建议饮用水安全应有专管部门本报北京６月２８日电（记者白龙...  \n",
       "84  零点青年公益创业发展中心（ＹＥＳ）发起“白领黑苹果”【春夏秋冬】四季会系列活动，旨在联系关注...  \n",
       "85  ２０１０年１０月，广发希望慈善基金的志愿者代表们带着代表希望的“心愿卡”深入到云南楚雄彝族自...  \n",
       "86  世界将在本月３１日迎来第７０亿人口，而在３１日凌晨，将成为象征性的全球第７０亿名成员之一的婴...  \n",
       "87  索尼海外学生交流计划（Ｓｏｎｙ　Ｓｔｕｄｅｎｔ　Ｐｒｏｊｅｃｔ　Ａｂｒｏａｄ　（Ｃｈｉｎａ）...  \n",
       "88  爱纯是一个出生在中国北部地区的小女孩。笑容背后，她还是一个患有心脏病的孤儿。她的心脏室间隔缺...  \n",
       "89  欢迎各企业动员本企业员工和／或合作伙伴一同参与本次活动并进行捐赠。每位参与活动人员的注册费用...  \n",
       "90  国人出国旅游买回象牙属违法，三年来涉案金额过亿；私带象牙制品入境，轻则罚没重则追究刑事责任。...  \n",
       "91  在许多情况下，只有公正和中立的组织才有能力为被武装冲突波及的人们提供帮助。例如，２００３年３...  \n",
       "92  活动主办：２１世纪经济报道战略合作：东风标致５０８我们深信：知识改变命运。乡村的孩子们跟...  \n",
       "93  “爱鸟周”源于１９８１年，最初为保护迁徙于中日两国间的候鸟而设立。１９９２年国务院批准的《陆...  \n",
       "94  延迟退休拟采取差别政策人社部表示将以适当方式充分听取各方意见，并以＂小步慢走＂的方式实施...  \n",
       "95  导读：几万年前，世界野外大熊猫种群分布最南端在缅甸和越南北部北纬１９度一带，由于气候的变迁和...  \n",
       "96  今年５月１０日，备受关注的“合肥市少女毁容案”在合肥市市包河区法院第一法庭公开宣判，但此案附...  \n",
       "97  ＳＯＨＯＣＨＩＮＡ向项目捐赠３３８００元“给孩子送双运动鞋”公益行动由中华少年儿童慈善救助...  \n",
       "98  红会首次发遇难者慰问金遇难群众每人１万　因公殉职每人２万　烈士每人３万本报讯（记者李秋萌...  \n",
       "99  截止目前，通过搜狐官网和溜达音乐网提交的参赛词、曲作品已达５００多首。因参赛者的强烈要求，...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"data.csv\")\n",
    "reviews.columns = ['summarization','article']\n",
    "reviews.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# source_file = './data/evaluation_with_ground_truth.txt'\n",
    "# reviews = pd.DataFrame.from_dict({\"summarization\":\"\",\"article\":\"\",\"index\":\"\"},orient='index').T\n",
    "# with open(source_file, 'r', encoding='utf-8') as f:\n",
    "#     source_data = f.read()\n",
    "# i = 0\n",
    "# for line in source_data.split('\\n'):\n",
    "#     line = eval(line)\n",
    "#     reviews.loc[i] = line\n",
    "#     i = i + 1\n",
    "#     if i == 2000:\n",
    "#         break\n",
    "# reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summarization</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>深圳地铁将设立ＶＩＰ头等车厢　买双倍票可享坐票</td>\n",
       "      <td>南都讯　记者刘凡　周昌和　任笑一　继推出日票后，深圳今后将设地铁ＶＩＰ头等车厢，设坐票制。昨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中国西部是地球上主要干旱带之一，妇女是当地劳动力．．．</td>\n",
       "      <td>同心县地处宁夏中部干旱带的核心区，　冬寒长，春暖迟，夏热短，秋凉早，干旱少雨，蒸发强烈，风大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>思源焦点公益基金救助孩子：永康</td>\n",
       "      <td>不满一岁的永康是个饱经病痛折磨的孩子，２０１１年７月５日出生的他，患有先天性心脏病、疝气，一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>康师傅回应转卖废弃茶叶：下家承诺用废料做枕头</td>\n",
       "      <td>就废弃茶叶被转手事件发声明本报讯（记者刘俊）　“我们也是受害者！”昨日，有媒体报道称康师傅...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>活动时间：</td>\n",
       "      <td>·奖励办法：率先提交的前１００个创意项目，经评估，可优先资助实施。·咨询电话：０１０－６７...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 summarization  \\\n",
       "0      深圳地铁将设立ＶＩＰ头等车厢　买双倍票可享坐票   \n",
       "1  中国西部是地球上主要干旱带之一，妇女是当地劳动力．．．   \n",
       "2              思源焦点公益基金救助孩子：永康   \n",
       "3       康师傅回应转卖废弃茶叶：下家承诺用废料做枕头   \n",
       "4                        活动时间：   \n",
       "\n",
       "                                             article  \n",
       "0  南都讯　记者刘凡　周昌和　任笑一　继推出日票后，深圳今后将设地铁ＶＩＰ头等车厢，设坐票制。昨...  \n",
       "1  同心县地处宁夏中部干旱带的核心区，　冬寒长，春暖迟，夏热短，秋凉早，干旱少雨，蒸发强烈，风大...  \n",
       "2  不满一岁的永康是个饱经病痛折磨的孩子，２０１１年７月５日出生的他，患有先天性心脏病、疝气，一...  \n",
       "3  就废弃茶叶被转手事件发声明本报讯（记者刘俊）　“我们也是受害者！”昨日，有媒体报道称康师傅...  \n",
       "4  ·奖励办法：率先提交的前１００个创意项目，经评估，可优先资助实施。·咨询电话：０１０－６７...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews = pd.read_table('./data/evaluation_with_ground_truth.txt',names=['summarization','article','index'],encoding='utf-8')\n",
    "reviews.isnull().sum()\n",
    "reviews = reviews.dropna()\n",
    "# reviews = reviews.drop(['index'], 1)\n",
    "reviews = reviews.reset_index(drop=True)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text,remove_stopwords=True):\n",
    "#     if True:\n",
    "#         text = text.split()\n",
    "#         new_text = []\n",
    "#         for word in text:\n",
    "#             if word in contractions:\n",
    "#                 new_text.append(contractions[word])\n",
    "#             else:\n",
    "#                 new_text.append(word)\n",
    "#         text = \" \".join(new_text)\n",
    "    \n",
    "    text = re.sub(r'<Paragraph>', '', text) \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', '', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', '', text)\n",
    "    text = re.sub(r'<br />', '', text)\n",
    "    text = re.sub(r'\\'', '', text) \n",
    "    \n",
    "    text = jieba.cut(text)\n",
    "    if text != '\\r\\n': #换行符\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "#     text_list = list(text)\n",
    "#     text = \" \".join(text_list)\n",
    "    \n",
    "            \n",
    "    if remove_stopwords:\n",
    "        stopwords = pd.read_csv(\"stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')    \n",
    "        stopwords = stopwords.stopword.values.tolist()\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stopwords]\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.574 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b25155cd0977>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclean_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mclean_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-b7eaf663064a>\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(text, remove_stopwords)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stopwords.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stopword'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m   7319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7320\u001b[0m     \u001b[1;31m# from BlockManager perspective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7321\u001b[1;33m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7323\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_ensure_index\u001b[1;34m(index_like, copy)\u001b[0m\n\u001b[0;32m   4941\u001b[0m             \u001b[0mindex_like\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4943\u001b[1;33m         \u001b[0mconverted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_index_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall_arrays\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.clean_index_list\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.infer_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clean_summaries = []\n",
    "for summary in reviews.summarization:\n",
    "    clean_summaries.append(clean_text(summary,remove_stopwords=False))\n",
    "clean_texts = []\n",
    "for text in reviews.article:\n",
    "    clean_texts.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(clean_summaries[i])\n",
    "    print(clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict,text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts,clean_summaries)\n",
    "count_words(word_counts,clean_texts)\n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'sgns.sogounews.bigram-char'\n",
    "topn = 100\n",
    "def read_vectors(path, topn):  # read top n word vectors, i.e. top is 10000\n",
    "    lines_num, dim = 0, 0\n",
    "    vectors = {}\n",
    "    iw = []\n",
    "    wi = {}\n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        first_line = True\n",
    "        for line in f:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                dim = int(line.rstrip().split()[1])\n",
    "                continue\n",
    "            lines_num += 1\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "            iw.append(tokens[0])\n",
    "            if topn != 0 and lines_num >= topn:\n",
    "                break\n",
    "    for i, w in enumerate(iw):\n",
    "        wi[w] = i\n",
    "    return vectors, iw, wi, dim\n",
    "embeddings_index, iw, wi, dim = read_vectors(path, topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_words = 0\n",
    "threshold = 2\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {}\n",
    "value = 0\n",
    "for word,count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value +=1\n",
    "        \n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)    \n",
    "    \n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 6200\n",
    "max_summary_length = 40\n",
    "min_length = 2\n",
    "# unk_text_limit = 1\n",
    "# unk_summary_limit = 0\n",
    "# print (int_summaries)\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "#             unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "#             unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits,_,_  = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits,_,_  = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    # creating Dense- This is also called output layer. This will produce the summary.\n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    # Using BahdanauAttention as one of the widely used Attention Algorithms\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "    \n",
    "    # initializing the initial state, layer it would be update by output from one cell        \n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "    # Creating training logits - which would be used during training dataset\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "    # Creating inference logits - which would produce output using train model\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 5\n",
      "The longest text length: 354\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 0\n",
    "end = start + 1500\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for this update: 11.409\n",
      "New Record!\n",
      "Average loss for this update: 6.175\n",
      "New Record!\n",
      "Average loss for this update: 6.085\n",
      "New Record!\n",
      "Epoch   1/100 Batch   20/23 - Loss:  7.688, Seconds: 16.44\n",
      "Average loss for this update: 6.27\n",
      "No Improvement.\n",
      "Average loss for this update: 5.244\n",
      "New Record!\n",
      "Average loss for this update: 5.35\n",
      "No Improvement.\n",
      "Epoch   2/100 Batch   20/23 - Loss:  5.575, Seconds: 15.88\n",
      "Average loss for this update: 5.701\n",
      "No Improvement.\n",
      "Average loss for this update: 4.966\n",
      "New Record!\n",
      "Average loss for this update: 5.084\n",
      "No Improvement.\n",
      "Epoch   3/100 Batch   20/23 - Loss:  5.207, Seconds: 15.88\n",
      "Average loss for this update: 5.249\n",
      "No Improvement.\n",
      "Average loss for this update: 4.69\n",
      "New Record!\n",
      "Average loss for this update: 4.868\n",
      "No Improvement.\n",
      "Epoch   4/100 Batch   20/23 - Loss:  4.903, Seconds: 16.48\n",
      "Average loss for this update: 4.896\n",
      "No Improvement.\n",
      "Average loss for this update: 4.42\n",
      "New Record!\n",
      "Average loss for this update: 4.645\n",
      "No Improvement.\n",
      "Epoch   5/100 Batch   20/23 - Loss:  4.634, Seconds: 15.83\n",
      "Average loss for this update: 4.504\n",
      "No Improvement.\n",
      "Average loss for this update: 4.17\n",
      "New Record!\n",
      "Average loss for this update: 4.271\n",
      "No Improvement.\n",
      "Epoch   6/100 Batch   20/23 - Loss:  4.307, Seconds: 16.22\n",
      "Average loss for this update: 4.067\n",
      "New Record!\n",
      "Average loss for this update: 3.79\n",
      "New Record!\n",
      "Average loss for this update: 3.917\n",
      "No Improvement.\n",
      "Epoch   7/100 Batch   20/23 - Loss:  3.917, Seconds: 16.46\n",
      "Average loss for this update: 3.606\n",
      "New Record!\n",
      "Average loss for this update: 3.398\n",
      "New Record!\n",
      "Average loss for this update: 3.563\n",
      "No Improvement.\n",
      "Epoch   8/100 Batch   20/23 - Loss:  3.524, Seconds: 16.36\n",
      "Average loss for this update: 3.364\n",
      "New Record!\n",
      "Average loss for this update: 3.034\n",
      "New Record!\n",
      "Average loss for this update: 3.11\n",
      "No Improvement.\n",
      "Epoch   9/100 Batch   20/23 - Loss:  3.154, Seconds: 16.46\n",
      "Average loss for this update: 3.059\n",
      "No Improvement.\n",
      "Average loss for this update: 2.711\n",
      "New Record!\n",
      "Average loss for this update: 2.734\n",
      "No Improvement.\n",
      "Epoch  10/100 Batch   20/23 - Loss:  2.818, Seconds: 16.44\n",
      "Average loss for this update: 2.716\n",
      "No Improvement.\n",
      "Average loss for this update: 2.332\n",
      "New Record!\n",
      "Average loss for this update: 2.44\n",
      "No Improvement.\n",
      "Epoch  11/100 Batch   20/23 - Loss:  2.477, Seconds: 16.24\n",
      "Average loss for this update: 2.492\n",
      "No Improvement.\n",
      "Average loss for this update: 2.043\n",
      "New Record!\n",
      "Average loss for this update: 2.079\n",
      "No Improvement.\n",
      "Epoch  12/100 Batch   20/23 - Loss:  2.179, Seconds: 16.22\n",
      "Average loss for this update: 2.247\n",
      "No Improvement.\n",
      "Average loss for this update: 1.812\n",
      "New Record!\n",
      "Average loss for this update: 1.82\n",
      "No Improvement.\n",
      "Epoch  13/100 Batch   20/23 - Loss:  1.933, Seconds: 16.30\n",
      "Average loss for this update: 1.995\n",
      "No Improvement.\n",
      "Average loss for this update: 1.551\n",
      "New Record!\n",
      "Average loss for this update: 1.546\n",
      "New Record!\n",
      "Epoch  14/100 Batch   20/23 - Loss:  1.674, Seconds: 16.30\n",
      "Average loss for this update: 1.731\n",
      "No Improvement.\n",
      "Average loss for this update: 1.364\n",
      "New Record!\n",
      "Average loss for this update: 1.364\n",
      "No Improvement.\n",
      "Epoch  15/100 Batch   20/23 - Loss:  1.462, Seconds: 16.12\n",
      "Average loss for this update: 1.626\n",
      "No Improvement.\n",
      "Average loss for this update: 1.315\n",
      "New Record!\n",
      "Average loss for this update: 1.248\n",
      "New Record!\n",
      "Epoch  16/100 Batch   20/23 - Loss:  1.373, Seconds: 16.62\n",
      "Average loss for this update: 1.444\n",
      "No Improvement.\n",
      "Average loss for this update: 1.134\n",
      "New Record!\n",
      "Average loss for this update: 1.112\n",
      "New Record!\n",
      "Epoch  17/100 Batch   20/23 - Loss:  1.209, Seconds: 16.00\n",
      "Average loss for this update: 1.245\n",
      "No Improvement.\n",
      "Average loss for this update: 1.021\n",
      "New Record!\n",
      "Average loss for this update: 0.967\n",
      "New Record!\n",
      "Epoch  18/100 Batch   20/23 - Loss:  1.061, Seconds: 16.60\n",
      "Average loss for this update: 1.118\n",
      "No Improvement.\n",
      "Average loss for this update: 0.903\n",
      "New Record!\n",
      "Average loss for this update: 0.866\n",
      "New Record!\n",
      "Epoch  19/100 Batch   20/23 - Loss:  0.948, Seconds: 16.00\n",
      "Average loss for this update: 0.963\n",
      "No Improvement.\n",
      "Average loss for this update: 0.816\n",
      "New Record!\n",
      "Average loss for this update: 0.808\n",
      "New Record!\n",
      "Epoch  20/100 Batch   20/23 - Loss:  0.851, Seconds: 16.42\n",
      "Average loss for this update: 0.893\n",
      "No Improvement.\n",
      "Average loss for this update: 0.738\n",
      "New Record!\n",
      "Average loss for this update: 0.723\n",
      "New Record!\n",
      "Epoch  21/100 Batch   20/23 - Loss:  0.775, Seconds: 16.84\n",
      "Average loss for this update: 0.864\n",
      "No Improvement.\n",
      "Average loss for this update: 0.667\n",
      "New Record!\n",
      "Average loss for this update: 0.634\n",
      "New Record!\n",
      "Epoch  22/100 Batch   20/23 - Loss:  0.711, Seconds: 16.10\n",
      "Average loss for this update: 0.8\n",
      "No Improvement.\n",
      "Average loss for this update: 0.621\n",
      "New Record!\n",
      "Average loss for this update: 0.568\n",
      "New Record!\n",
      "Epoch  23/100 Batch   20/23 - Loss:  0.651, Seconds: 15.60\n",
      "Average loss for this update: 0.736\n",
      "No Improvement.\n",
      "Average loss for this update: 0.56\n",
      "New Record!\n",
      "Average loss for this update: 0.527\n",
      "New Record!\n",
      "Epoch  24/100 Batch   20/23 - Loss:  0.595, Seconds: 16.30\n",
      "Average loss for this update: 0.712\n",
      "No Improvement.\n",
      "Average loss for this update: 0.491\n",
      "New Record!\n",
      "Average loss for this update: 0.491\n",
      "New Record!\n",
      "Epoch  25/100 Batch   20/23 - Loss:  0.554, Seconds: 16.08\n",
      "Average loss for this update: 0.612\n",
      "No Improvement.\n",
      "Average loss for this update: 0.433\n",
      "New Record!\n",
      "Average loss for this update: 0.436\n",
      "No Improvement.\n",
      "Epoch  26/100 Batch   20/23 - Loss:  0.483, Seconds: 16.52\n",
      "Average loss for this update: 0.575\n",
      "No Improvement.\n",
      "Average loss for this update: 0.428\n",
      "New Record!\n",
      "Average loss for this update: 0.401\n",
      "New Record!\n",
      "Epoch  27/100 Batch   20/23 - Loss:  0.460, Seconds: 16.36\n",
      "Average loss for this update: 0.519\n",
      "No Improvement.\n",
      "Average loss for this update: 0.412\n",
      "No Improvement.\n",
      "Average loss for this update: 0.378\n",
      "New Record!\n",
      "Epoch  28/100 Batch   20/23 - Loss:  0.431, Seconds: 16.42\n",
      "Average loss for this update: 0.489\n",
      "No Improvement.\n",
      "Average loss for this update: 0.392\n",
      "No Improvement.\n",
      "Average loss for this update: 0.366\n",
      "New Record!\n",
      "Epoch  29/100 Batch   20/23 - Loss:  0.408, Seconds: 16.32\n",
      "Average loss for this update: 0.459\n",
      "No Improvement.\n",
      "Average loss for this update: 0.365\n",
      "New Record!\n",
      "Average loss for this update: 0.353\n",
      "New Record!\n",
      "Epoch  30/100 Batch   20/23 - Loss:  0.387, Seconds: 16.36\n",
      "Average loss for this update: 0.484\n",
      "No Improvement.\n",
      "Average loss for this update: 0.367\n",
      "No Improvement.\n",
      "Average loss for this update: 0.337\n",
      "New Record!\n",
      "Epoch  31/100 Batch   20/23 - Loss:  0.387, Seconds: 16.06\n",
      "Average loss for this update: 0.474\n",
      "No Improvement.\n",
      "Average loss for this update: 0.35\n",
      "No Improvement.\n",
      "Average loss for this update: 0.324\n",
      "New Record!\n",
      "Epoch  32/100 Batch   20/23 - Loss:  0.375, Seconds: 16.14\n",
      "Average loss for this update: 0.491\n",
      "No Improvement.\n",
      "Average loss for this update: 0.337\n",
      "No Improvement.\n",
      "Average loss for this update: 0.324\n",
      "No Improvement.\n",
      "Epoch  33/100 Batch   20/23 - Loss:  0.377, Seconds: 16.44\n",
      "Average loss for this update: 0.435\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 4 # If the update loss does not decrease in 4 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "#     loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "#     loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Original Text: 福州 新闻 海都网 环卫工人 节日 享用 海鲜大餐 志华摄 昨天 五一劳动节 劳动者 节日 昨天中午 扬州 市区 一家 名叫 海盗 船长 平价 海鲜 饭店 老板 蒋军 邀请 邗江 环卫所 名 一线 环卫工人 免费 吃 海鲜大餐 方式 城市 美容师 致敬 招待 这批 环卫 贵宾 蒋 老板 婉拒 前去 就餐 客人 环卫工 第一次 走进 海鲜 酒店 昨天上午 点 海盗 船长 饭店 一楼 大厅 墙上 挂 热烈欢迎 环卫工 师傅 莅临 海盗 船长 用餐 横幅 乖乖 热烈欢迎 环卫工 师傅 请客 饭店 老板 环卫工 尊重 字 热乎乎 岁 环卫工 马五喜 说出 感受 饭店 大厅 环卫工 包场 中心 位置 放上 张大 圆桌 招待 环卫工 大厅 南墙 零星 卡座 接待 散客 上菜 糖醋排骨 鸭肫 时鲜 毛豆 道 冷菜 桌 啤酒 饮料 自选 倒 饮料 啤酒 举起 杯子 庆祝 节日 油淋 大虾 蒜 茸 粉丝 蒸 十头 鲍 家烧 缩骨鱼 辣 炒 花甲 姜葱 炒 蛏子 文蛤 蒸 鸡蛋 道 热菜 点心 精美 水果 拼盘 大伙 一饱口福 绝大多数 环卫工 这是 平生 第一次 进 海鲜 大酒店 吃饭 天前 接到 所长 关照 吃饭 通知 激动 想不到 干 环卫 受 尊重 小常 母亲 告诉 干 环卫 第一次 收到 邀请 环卫工 辛苦 总想 做点 劳动节 当天 邀请 名 环卫工 吃饭 一点 自我 炒作 想法 表达 尊重 平时 起早贪黑 扫 马路 实在 辛苦 老板 蒋军 介绍 平时 路上 开车 环卫工 天不亮 扫 马路 捡 马路 中央 垃圾 穿梭 车流 中 辛苦 危险 做点 想法 烟花 三月 经贸 旅游节 前 扬州 中外 嘉宾 打造 洁净 市容 环境 白天 忙碌 外 深夜 路面 车流 少时 冲洗 马路 触动 劳动节 期间 请 环卫工 吃饭 劳动节 当天 酒店 忙 避开 劳动节 当天 请 劳动节 当天 请 表达 环卫工 敬意 折扣 婉拒 客人 得知 请 环卫工 吃饭 理解 环卫工 吃 酒店 工作人员 环卫工 热情服务 菜谱 每桌 1800 元 标准 安排 标准 酒店 中 高档次 环卫工 吃饭 真的 这位 老板 爱心 感动 环卫工 相互 敬酒 热闹 场景 坐在 旁边 就餐 散客 周先生 感染\n",
      "\n",
      "Text\n",
      "  Word Ids:    [201, 1939, 15174, 8106, 1972, 41228, 34653, 41228, 2280, 41228, 34013, 1972, 18280, 4990, 1134, 1271, 5580, 30870, 24648, 22013, 8103, 7497, 1202, 34654, 5524, 8105, 22305, 692, 15826, 8106, 808, 1407, 34653, 1304, 5567, 18592, 7396, 34353, 3309, 4864, 34655, 27797, 1202, 8104, 20398, 5703, 1981, 1973, 13895, 13535, 8103, 120, 14833, 1288, 30870, 24648, 7497, 1697, 1698, 16801, 4384, 34656, 1973, 13393, 41228, 30870, 24648, 21285, 27804, 23827, 34656, 1973, 13393, 8183, 7497, 1202, 1973, 13747, 12446, 30040, 404, 1973, 41228, 18588, 13107, 7497, 1698, 1973, 26788, 502, 11191, 1370, 41228, 34657, 34353, 1973, 1698, 41228, 21082, 34658, 2474, 34659, 41228, 41228, 41228, 41228, 41228, 6892, 41228, 34660, 10129, 13505, 41228, 522, 13505, 10129, 6748, 14118, 1569, 1972, 41228, 41228, 11405, 41228, 1940, 22655, 41228, 41228, 41228, 41228, 33279, 30753, 34661, 41228, 30753, 41228, 41228, 22655, 7547, 6892, 13507, 32814, 34662, 6914, 41228, 18589, 41228, 12772, 1973, 3683, 41228, 13895, 2066, 8103, 13591, 786, 13950, 5355, 5538, 14178, 786, 2580, 918, 23579, 4371, 4864, 367, 13747, 41228, 2351, 7645, 4371, 4864, 13895, 3656, 5524, 1973, 8108, 856, 2697, 8110, 34663, 1998, 5524, 692, 1973, 786, 3936, 16028, 8588, 14027, 11083, 13747, 6093, 34664, 28717, 12320, 13787, 8108, 1202, 34654, 3061, 6093, 1808, 2818, 1973, 34665, 28717, 12320, 4317, 12320, 4155, 523, 29923, 31388, 1219, 8108, 3192, 8110, 14027, 12279, 34666, 16828, 41228, 441, 4990, 34667, 7067, 1344, 28889, 11270, 13981, 247, 17504, 7844, 2164, 3765, 31388, 41228, 28128, 12320, 34362, 34663, 2719, 259, 1973, 786, 34663, 1998, 120, 13710, 33098, 34663, 1998, 259, 34663, 1998, 259, 11083, 1973, 28091, 11061, 8104, 1981, 12792, 259, 1973, 786, 8420, 1973, 1407, 120, 4301, 1973, 41228, 41228, 41228, 7920, 585, 1323, 4983, 1323, 120, 1219, 41228, 1973, 786, 13044, 13482, 1202, 12783, 2501, 1973, 9398, 34668, 13255, 4454, 15432, 1562, 5703, 34659, 26886, 3068]\n",
      "  Input Words: 福州 新闻 海都网 环卫工人 节日 <UNK> 海鲜大餐 <UNK> 昨天 <UNK> 劳动者 节日 昨天中午 扬州 市区 一家 名叫 海盗 船长 平价 海鲜 饭店 老板 蒋军 邀请 邗江 环卫所 名 一线 环卫工人 免费 吃 海鲜大餐 方式 城市 美容师 致敬 招待 这批 环卫 贵宾 蒋 老板 婉拒 前去 就餐 客人 环卫工 第一次 走进 海鲜 酒店 昨天上午 点 海盗 船长 饭店 一楼 大厅 墙上 挂 热烈欢迎 环卫工 师傅 <UNK> 海盗 船长 用餐 横幅 乖乖 热烈欢迎 环卫工 师傅 请客 饭店 老板 环卫工 尊重 字 热乎乎 岁 环卫工 <UNK> 说出 感受 饭店 大厅 环卫工 包场 中心 位置 放 <UNK> 圆桌 招待 环卫工 大厅 <UNK> 零星 卡座 接待 散客 <UNK> <UNK> <UNK> <UNK> <UNK> 道 <UNK> 桌 啤酒 饮料 <UNK> 倒 饮料 啤酒 举起 杯子 庆祝 节日 <UNK> <UNK> 蒜 <UNK> 粉丝 蒸 <UNK> <UNK> <UNK> <UNK> 辣 炒 花甲 <UNK> 炒 <UNK> <UNK> 蒸 鸡蛋 道 热菜 点心 精美 水果 <UNK> 大伙 <UNK> 绝大多数 环卫工 这是 <UNK> 第一次 进 海鲜 大酒店 吃饭 天前 接到 所长 关照 吃饭 通知 激动 想不到 干 环卫 受 尊重 <UNK> 母亲 告诉 干 环卫 第一次 收到 邀请 环卫工 辛苦 总 想 做点 劳动节 当天 邀请 名 环卫工 吃饭 一点 自我 炒作 想法 表达 尊重 平时 起早贪黑 扫 马路 实在 辛苦 老板 蒋军 介绍 平时 路上 开车 环卫工 天不亮 扫 马路 捡 马路 中央 垃圾 穿梭 车流 中 辛苦 危险 做点 想法 烟花 三月 经贸 <UNK> 前 扬州 中外 嘉宾 打造 洁净 市容 环境 白天 忙碌 外 深夜 路面 车流 <UNK> 冲洗 马路 触动 劳动节 期间 请 环卫工 吃饭 劳动节 当天 酒店 忙 避开 劳动节 当天 请 劳动节 当天 请 表达 环卫工 敬意 折扣 婉拒 客人 得知 请 环卫工 吃饭 理解 环卫工 吃 酒店 工作人员 环卫工 <UNK> <UNK> <UNK> 1800 元 标准 安排 标准 酒店 中 <UNK> 环卫工 吃饭 真的 这位 老板 爱心 感动 环卫工 相互 敬酒 热闹 场景 坐在 旁边 就餐 散客 周先生 感染\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [4990, 88, 8103, 7497, 1202, 8104, 5703]\n",
      "  Response Words: 扬州 ： 海鲜 饭店 老板 婉拒 就餐\n"
     ]
    }
   ],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "#input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
    "                  #I think that I will try a green apple next time.\"\n",
    "#text = text_to_seq(input_sentence)\n",
    "random = np.random.randint(0,len(clean_texts))\n",
    "input_sentence = clean_texts[1006]\n",
    "text = text_to_seq(clean_texts[1006])\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
